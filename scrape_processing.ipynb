{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from textblob import TextBlob, Word\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86ff1ea1-0b63-43ce-addc-eb43f6193b3b</td>\n",
       "      <td>Yaseen Yaseen</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ALm5wu...</td>\n",
       "      <td>Yaeen Yaeen gg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-10-04 20:32:28</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3577f7a1-3394-4e77-813d-095a82cf8bcf</td>\n",
       "      <td>Kemar Richardson</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ACNPE...</td>\n",
       "      <td>Great</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>26.3.4</td>\n",
       "      <td>2022-10-04 20:32:10</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7c8c56d9-d8ad-47d4-b24b-5289aa4529ff</td>\n",
       "      <td>Tracy Dunn</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ALm5wu...</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>26.4.3</td>\n",
       "      <td>2022-10-04 20:31:21</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80db804f-cccd-4b09-b690-abc12cbf0612</td>\n",
       "      <td>SG. Mugo. (Mugoz:)</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ACNPE...</td>\n",
       "      <td>Good app</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>26.3.4</td>\n",
       "      <td>2022-10-04 20:30:22</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4ed35e90-0f45-4865-81c4-b3a6f2ea49f7</td>\n",
       "      <td>Mwansa Judy</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ACNPE...</td>\n",
       "      <td>Most amazing app</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>26.3.4</td>\n",
       "      <td>2022-10-04 20:29:25</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId            userName  \\\n",
       "0  86ff1ea1-0b63-43ce-addc-eb43f6193b3b       Yaseen Yaseen   \n",
       "1  3577f7a1-3394-4e77-813d-095a82cf8bcf    Kemar Richardson   \n",
       "2  7c8c56d9-d8ad-47d4-b24b-5289aa4529ff          Tracy Dunn   \n",
       "3  80db804f-cccd-4b09-b690-abc12cbf0612  SG. Mugo. (Mugoz:)   \n",
       "4  4ed35e90-0f45-4865-81c4-b3a6f2ea49f7         Mwansa Judy   \n",
       "\n",
       "                                           userImage           content  score  \\\n",
       "0  https://play-lh.googleusercontent.com/a/ALm5wu...    Yaeen Yaeen gg      5   \n",
       "1  https://play-lh.googleusercontent.com/a-/ACNPE...             Great      5   \n",
       "2  https://play-lh.googleusercontent.com/a/ALm5wu...              good      5   \n",
       "3  https://play-lh.googleusercontent.com/a-/ACNPE...          Good app      5   \n",
       "4  https://play-lh.googleusercontent.com/a-/ACNPE...  Most amazing app      5   \n",
       "\n",
       "   thumbsUpCount reviewCreatedVersion                  at replyContent  \\\n",
       "0              0                 None 2022-10-04 20:32:28         None   \n",
       "1              0               26.3.4 2022-10-04 20:32:10         None   \n",
       "2              0               26.4.3 2022-10-04 20:31:21         None   \n",
       "3              0               26.3.4 2022-10-04 20:30:22         None   \n",
       "4              0               26.3.4 2022-10-04 20:29:25         None   \n",
       "\n",
       "  repliedAt  \n",
       "0       NaT  \n",
       "1       NaT  \n",
       "2       NaT  \n",
       "3       NaT  \n",
       "4       NaT  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in pickle\n",
    "with open('result2.pkl', 'rb') as f:\n",
    "    result = pickle.load(f)\n",
    "\n",
    "# convert to dataframe and store as pickle\n",
    "df = pd.DataFrame(result)\n",
    "df.to_pickle('df.pkl')\n",
    "\n",
    "# print df head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewId', 'userName', 'userImage', 'content', 'score',\n",
       "       'thumbsUpCount', 'reviewCreatedVersion', 'at', 'replyContent',\n",
       "       'repliedAt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check fields inside df\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewId                14178\n",
       "userName                14178\n",
       "userImage               14178\n",
       "content                 14178\n",
       "score                   14178\n",
       "thumbsUpCount           14178\n",
       "reviewCreatedVersion     9491\n",
       "at                      14178\n",
       "replyContent               13\n",
       "repliedAt                  13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many reviews with score of 1\n",
    "df[df.score == 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement functions to clean up the reviews, we will want to remove unnecesary stop words, punctuations @mentions, #hashtags, emojis, emails etc.\n",
    "We will also perform english detection and spelling correction to ensure the correctedness of the cleaned data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuations(review: str) -> str:\n",
    "    words: list[str] = review.split()\n",
    "    table: dict[int, int] = str.maketrans('', '', string.punctuation)\n",
    "    stripped: list[str] = [w.translate(table) for w in words]\n",
    "    return \" \".join(stripped)\n",
    "\n",
    "def normalize(review: str) -> str:\n",
    "    return review.lower()\n",
    "\n",
    "def remove_stopwords(review: str, stopwords: set[str]) -> str:\n",
    "    words: list[str] = review.split()\n",
    "    words = [w for w in words if not w in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def stemming(review: str) -> str:\n",
    "    tokens: list[str] = word_tokenize(review)\n",
    "    porter: PorterStemmer = PorterStemmer()\n",
    "    stemmed: list[str] = [porter.stem(word) for word in tokens]\n",
    "    return \" \".join(stemmed)\n",
    "\n",
    "def lemmatize(review: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens: list[str] = word_tokenize(review)\n",
    "    lemmatized: list[str] = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "def depure_data(data):\n",
    "    \n",
    "    #remove url with a RegEx\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    #remove email address\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    #remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    #remove single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def detokenize(text):\n",
    "    #treebank tokenizer uses RegEx to tokenize text as in Penn Treebank\n",
    "    return TreebankWordDetokenizer().detokenize(text)\n",
    "\n",
    "\n",
    "def sent_to_words(sentence):\n",
    "    return gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "        \n",
    "def correct_word_spelling(word):\n",
    "    word = Word(word)\n",
    "    result = word.correct()\n",
    "    return result\n",
    "\n",
    "def correct_sentence(sentence: list[str], spell):\n",
    "    for j in range(len(sentence)):\n",
    "        word = sentence[j]\n",
    "\n",
    "        #use regex to reduce 3 consecutive letters to 2\n",
    "        reduced_word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "        sentence[j] = reduced_word\n",
    "\n",
    "        #if incorrect spelled, correct word\n",
    "        if sentence[j] != \"tiktok\" and len(spell.unknown([sentence[j]])) > 0:\n",
    "            sentence[j] = correct_word_spelling(sentence[j])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review: str, stopwords: set[str]) -> str:\n",
    "    result: str = depure_data(remove_stopwords(normalize(remove_punctuations(review)), stopwords))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: DataFrame):\n",
    "    # df['content'] = df['content'].apply(lambda x: x if is_english(x) else '')\n",
    "    # df = df[df['content'] != '']\n",
    "    spell = SpellChecker()\n",
    "    review: list[str] = list(df[\"content\"])\n",
    "    stop_words: set[str] = set(stopwords.words('english'))\n",
    "    cleaned_reviews: list[str] = list()\n",
    "    for i, review in enumerate(review):\n",
    "        cleaned_review = clean_review(review, stop_words)\n",
    "        cleaned_review_in_words = sent_to_words(clean_review)\n",
    "        correct_sentence(cleaned_review_in_words, spell)\n",
    "        cleaned_review = detokenize(cleaned_review_in_words) \n",
    "        cleaned_reviews.append(cleaned_review)\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "    df[\"Clean_texts\"] = cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m clean_data(df)\n",
      "\u001b[1;32m/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb Cell 9\u001b[0m in \u001b[0;36mclean_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m cleaned_review \u001b[39m=\u001b[39m clean_review(review, stop_words)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m cleaned_review_in_words \u001b[39m=\u001b[39m sent_to_words(clean_review)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m correct_sentence(cleaned_review_in_words, spell)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m cleaned_review \u001b[39m=\u001b[39m detokenize(cleaned_review_in_words) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m cleaned_reviews\u001b[39m.\u001b[39mappend(cleaned_review)\n",
      "\u001b[1;32m/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb Cell 9\u001b[0m in \u001b[0;36mcorrect_sentence\u001b[0;34m(sentence, spell)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m#if incorrect spelled, correct word\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mif\u001b[39;00m sentence[j] \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtiktok\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(spell\u001b[39m.\u001b[39munknown([sentence[j]])) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     sentence[j] \u001b[39m=\u001b[39m correct_word_spelling(sentence[j])\n",
      "\u001b[1;32m/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb Cell 9\u001b[0m in \u001b[0;36mcorrect_word_spelling\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrect_word_spelling\u001b[39m(word):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     word \u001b[39m=\u001b[39m Word(word)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     result \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39;49mcorrect()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangxinghao/Desktop/CZ4045-Assignment/scrape_processing.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/textblob/blob.py:142\u001b[0m, in \u001b[0;36mWord.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    137\u001b[0m     \u001b[39m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m Word(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspellcheck()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/textblob/blob.py:134\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspellcheck\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    126\u001b[0m     \u001b[39m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[39m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m suggest(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/textblob/en/__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msuggest\u001b[39m(w):\n\u001b[1;32m    121\u001b[0m     \u001b[39m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m spelling\u001b[39m.\u001b[39;49msuggest(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/textblob/_text.py:1399\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m w\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misdigit():\n\u001b[1;32m   1396\u001b[0m     \u001b[39mreturn\u001b[39;00m [(w, \u001b[39m1.0\u001b[39m)] \u001b[39m# 1.5\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known([w]) \\\n\u001b[1;32m   1398\u001b[0m           \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w)) \\\n\u001b[0;32m-> 1399\u001b[0m           \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit2(w)) \\\n\u001b[1;32m   1400\u001b[0m           \u001b[39mor\u001b[39;00m [w]\n\u001b[1;32m   1401\u001b[0m candidates \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(c, \u001b[39m0.0\u001b[39m), c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m candidates]\n\u001b[1;32m   1402\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39msum\u001b[39m(p \u001b[39mfor\u001b[39;00m p, word \u001b[39min\u001b[39;00m candidates) \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/textblob/_text.py:1376\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39;49m(e2 \u001b[39mfor\u001b[39;49;00m e1 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit1(w) \u001b[39mfor\u001b[39;49;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit1(e1) \u001b[39mif\u001b[39;49;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/textblob/_text.py:1376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(e2 \u001b[39mfor\u001b[39;00m e1 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w) \u001b[39mfor\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(e1) \u001b[39mif\u001b[39;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/textblob/_text.py:96\u001b[0m, in \u001b[0;36mlazydict.__contains__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__contains__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy(\u001b[39m\"\u001b[39;49m\u001b[39m__contains__\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/textblob/_text.py:87\u001b[0m, in \u001b[0;36mlazydict._lazy\u001b[0;34m(self, method, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[1;32m     86\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, method, types\u001b[39m.\u001b[39mMethodType(\u001b[39mgetattr\u001b[39m(\u001b[39mdict\u001b[39m, method), \u001b[39mself\u001b[39m))\n\u001b[0;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mdict\u001b[39;49m, method)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langdetect import detect\n",
    "# def detect_my(text):\n",
    "#    try:\n",
    "#        return detect(text)\n",
    "#    except:\n",
    "#        return 'unknown'\n",
    "\n",
    "# # df['language'] = df['content'].apply(detect_my)\n",
    "\n",
    "# detect_my('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngliw\\AppData\\Local\\Temp\\ipykernel_18336\\165540621.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['content'] = df['content'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# return new column \"is_english\" with True or False\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return bool(re.search('[a-zA-Z]', text))\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "df['is_english'] = df['content'].apply(is_english)\n",
    "\n",
    "# remove puncutation\n",
    "df['content'] = df['content'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# save new df as pickle\n",
    "df.to_pickle('df_english.pkl')\n",
    "\n",
    "# save df to json\n",
    "df.to_json('df_english.json')\n",
    "\n",
    "# save df to csv\n",
    "df.to_csv('df_english.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
